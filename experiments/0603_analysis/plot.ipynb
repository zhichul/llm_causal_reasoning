{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "def load_metrics(file):\n",
    "    known_metrics = ['test_score_mean', 'test_score_sem', 'test_score_count', 'test_score_values']\n",
    "    metrics_parsed = defaultdict(dict)\n",
    "    with open(file, 'rt') as f:\n",
    "        metrics_raw = json.load(f)\n",
    "    for key in metrics_raw:\n",
    "        for known in known_metrics:\n",
    "            if key.startswith(f'{known}/'):\n",
    "                data_source = key[len(known)+1:]\n",
    "                metrics_parsed[data_source][known] = metrics_raw[key]\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            print(f'skipping {key}, unknown how to parse')\n",
    "    if len(metrics_parsed) == 1:\n",
    "        return next(iter(metrics_parsed.values()))\n",
    "    return metrics_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_line(linespec):\n",
    "    d = addict.Dict()\n",
    "    root = Path(f'generations') / Path(linespec.folder)\n",
    "    for file in root.glob(f'**/{linespec.step}/**/*.parquet.{linespec.reward_fn}.jsonl'):\n",
    "        spec, size, diff = file.parts[-4:-1]\n",
    "        split = file.stem.split(\".\")[0]\n",
    "        d[spec][size][diff][split] = load_metrics(file)\n",
    "    return d\n",
    "\n",
    "def load_lines(lines):\n",
    "    d = addict.Dict()\n",
    "    for line in lines:\n",
    "        d[line.name] = load_line(line)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from __future__ import annotations\n",
    "def latex_table(col_headers: List[str],\n",
    "                row_headers: List[str],\n",
    "                rows: List[List[Tuple[float, float]]],\n",
    "                digits: int = 2,\n",
    "                *,\n",
    "                times100: bool = False,\n",
    "                caption: str | None = None,\n",
    "                label: str | None = None,\n",
    "                use_booktabs: bool = True,\n",
    "                escape_underscores: bool = True,\n",
    "                table_font_size: str | None = None     # NEW\n",
    "                ) -> str:\n",
    "    \"\"\"\n",
    "    Build a LaTeX table from lists of (mean, std) tuples.\n",
    "\n",
    "    Features\n",
    "    --------\n",
    "    • Escapes underscores in headers / caption.\n",
    "    • Shows “± std” in \\\\tiny.\n",
    "    • Bold-faces the best mean in each row (highest value).\n",
    "    • Optional ×100 percentage display.\n",
    "    • Optional global font-size for the *whole table* (table_font_size).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_font_size : str | None\n",
    "        LaTeX size command *without backslash*, e.g. 'small', 'footnotesize'.\n",
    "        Wrapped around the tabular in a local group { ... }.\n",
    "    \"\"\"\n",
    "    if len(row_headers) != len(rows):\n",
    "        raise ValueError(\"row_headers length must equal number of row lists\")\n",
    "    for r in rows:\n",
    "        if len(r) != len(col_headers):\n",
    "            raise ValueError(\"each row must have len(col_headers) tuples\")\n",
    "\n",
    "    # ---------- helpers -----------------------------------------------------\n",
    "    esc = (lambda s: s.replace('_', r'\\_')) if escape_underscores else (lambda s: s)\n",
    "    fmt = f\"{{:.{digits}f}}\"\n",
    "\n",
    "    def _fmt(x: float) -> str:\n",
    "        return fmt.format(x * 100 if times100 else x)\n",
    "\n",
    "    def _cell(mean: float, std: float, bold=False) -> str:\n",
    "        txt = f\"{_fmt(mean)} {{\\\\tiny$\\\\pm$ {_fmt(std)}}}\"\n",
    "        return rf\"\\textbf{{{txt}}}\" if bold else txt\n",
    "\n",
    "    # ---------- build tabular ----------------------------------------------\n",
    "    border = (r\"\\toprule\", r\"\\midrule\", r\"\\bottomrule\") if use_booktabs \\\n",
    "             else (r\"\\hline\",) * 3\n",
    "    col_spec = \"l\" + \"c\" * len(col_headers)\n",
    "\n",
    "    lines = [\n",
    "        border[0],\n",
    "        \" & \".join([\"\"] + [esc(h) for h in col_headers]) + r\" \\\\\",\n",
    "        border[1],\n",
    "    ]\n",
    "\n",
    "    for rh, row in zip(row_headers, rows):\n",
    "        best_idx = max(range(len(row)), key=lambda i: row[i][0])\n",
    "        cells = [_cell(m, s, j == best_idx) for j, (m, s) in enumerate(row)]\n",
    "        lines.append(\" & \".join([esc(rh)] + cells) + r\" \\\\\")\n",
    "    lines.append(border[2])\n",
    "\n",
    "    tabular = '\\n'.join([\n",
    "        rf\"\\begin{{tabular}}{{{col_spec}}}\",\n",
    "        *lines,\n",
    "        r\"\\end{tabular}\"\n",
    "    ])\n",
    "\n",
    "    # ---------- optional font-size wrapper ----------------------------------\n",
    "    if table_font_size:\n",
    "        tabular = f\"{{\\\\{table_font_size}\\n{tabular}\\n}}\"\n",
    "\n",
    "    # ---------- optional full table environment -----------------------------\n",
    "    if caption or label:\n",
    "        parts = [r\"\\begin{table}[ht]\", r\"\\centering\", tabular]\n",
    "        if caption:\n",
    "            parts.append(rf\"\\caption{{{esc(caption)}}}\")\n",
    "        if label:\n",
    "            parts.append(rf\"\\label{{{label}}}\")\n",
    "        parts.append(r\"\\end{table}\")\n",
    "        return '\\n'.join(parts)\n",
    "\n",
    "    return tabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['specific_prompt_strict', 'specific_prompt_lenient', 'generic_prompt_strict', 'generic_prompt_lenient', 'specific_prompt_filter', 'generic_prompt_filter', 'specific_prompt_sft_loss', 'generic_prompt_sft_loss', 'specific_prompt_sft_reward', 'generic_prompt_sft_reward'])\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import addict\n",
    "\n",
    "with open('lines.yaml', 'rt') as f:\n",
    "    lines = [addict.Dict(d) for d in yaml.safe_load(f)]\n",
    "data = load_lines(lines)\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plot_config = {\n",
    "    \"lines\": \n",
    "    {\n",
    "        'specific_prompt_strict': {\n",
    "            'name': 'rl_strict_pspec',\n",
    "            'short': 'rl_stct_spc',\n",
    "            'color': 'red',\n",
    "            'linestyle': 'dashed'\n",
    "        },\n",
    "        'specific_prompt_filter': {\n",
    "            'name': 'rl_filter_pspec',\n",
    "            'short': 'rl_flt_spc',\n",
    "            'color': 'gold',\n",
    "            'linestyle': 'dashed'\n",
    "        },\n",
    "        'specific_prompt_lenient': {\n",
    "            'name': 'rl_soft_pspec',\n",
    "            'short': 'rl_sof_spc',\n",
    "            'color': 'orange',\n",
    "            'linestyle': 'dashed'\n",
    "        },\n",
    "        'generic_prompt_strict': {\n",
    "            'name': 'rl_strict_pgen',\n",
    "            'short': 'rl_stct_gen',\n",
    "            'color': 'red',\n",
    "            'linestyle': 'solid'\n",
    "        },\n",
    "        'generic_prompt_filter': {\n",
    "            'name': 'rl_filter_pgen',\n",
    "            'short': 'rl_flt_gen',\n",
    "            'color': 'gold',\n",
    "            'linestyle': 'solid'\n",
    "        },\n",
    "        'generic_prompt_lenient': {\n",
    "            'name': 'rl_soft_pgen',\n",
    "            'short': 'rl_sof_gen',\n",
    "            'color': 'orange',\n",
    "            'linestyle': 'solid'\n",
    "        },\n",
    "        'specific_prompt_sft_loss': {\n",
    "            'name': 'sft_loss_pspec',\n",
    "            'short': 'sft_los_spc',\n",
    "            'color': 'blue',\n",
    "            'linestyle': 'dashed'\n",
    "        },\n",
    "        'generic_prompt_sft_loss': {\n",
    "            'name': 'sft_loss_pgen',\n",
    "            'short': 'sft_los_gen',\n",
    "            'color': 'green',\n",
    "            'linestyle': 'solid'\n",
    "        },\n",
    "        'specific_prompt_sft_reward': {\n",
    "            'name': 'sft_reward_pspec',\n",
    "            'short': 'sft_rwd_spc',\n",
    "            'color': 'purple',\n",
    "            'linestyle': 'dashed'\n",
    "        },\n",
    "        'generic_prompt_sft_reward': {\n",
    "            'name': 'sft_reward_pgen',\n",
    "            'short': 'sft_rwd_gen',\n",
    "            'color': 'brown',\n",
    "            'linestyle': 'solid'\n",
    "        },\n",
    "    },\n",
    "    'ymin': 0.0,\n",
    "    'ymax': 1.0,\n",
    "    'shade_alpha': 0.2\n",
    "}\n",
    "\n",
    "def round_err(dat):\n",
    "    # return dat\n",
    "    return [round(100*d) / 100 for d in dat]\n",
    "\n",
    "tmax = 1.0\n",
    "tmin = 0.001\n",
    "for tmin, tmax, subname in [(0.001, 0.1, \"gt0.9\"), (0.001, 1.0, \"full\")]:\n",
    "    for spec in ['n10v2', 'n15v2']:\n",
    "        for size in ['2k']:\n",
    "            for d in ['d0', 'd1', 'd2', 'd3', 'd4', 'd5']:\n",
    "                plt.figure()\n",
    "                for line in plot_config['lines']:\n",
    "                    line_config = plot_config['lines'][line]\n",
    "                    x = 1-np.logspace(np.log10(tmin,),np.log10(tmax),100)\n",
    "                    if len(data[line]) == 0: raise ValueError(line)\n",
    "                    y_raw = (np.array(round_err(data[line][spec][size][d]['dev']['test_score_values']))[None,:] >  x[:,None])\n",
    "                    y_mean = y_raw.mean(-1)\n",
    "                    y_sem= y_raw.std(-1) / np.sqrt(y_raw.shape[-1])\n",
    "                    plt.plot(x, y_mean, \n",
    "                                linestyle=line_config['linestyle'], \n",
    "                                color=line_config['color'], \n",
    "                                label=line_config['name'],\n",
    "                            #      elinewidth=1,     # thinner bar lines\n",
    "                            # capsize=3,        # half-length of the “T” caps\n",
    "                            # capthick=1,        # thickness of the caps\n",
    "                    )\n",
    "                    plt.fill_between(x, y_mean-y_sem, y_mean+y_sem, color=line_config['color'], alpha=plot_config['shade_alpha'])\n",
    "                plt.legend()\n",
    "                plt.xlabel('threshold')\n",
    "                plt.ylabel('accuracy')\n",
    "                plt.title(f'{spec} {d} (n={data[line][spec][size][d][\"dev\"][\"test_score_count\"]})')\n",
    "                plt.ylim(plot_config['ymin'], plot_config['ymax'])\n",
    "                plt.savefig(f'plots/dev.{spec}.{size}.{d}.{subname}.threshold.pdf')\n",
    "                plt.clf()\n",
    "    for spec in ['n5v2']:\n",
    "        for size in ['2k']:\n",
    "            for d in ['d0', 'd1', 'd2', 'd3']:\n",
    "                plt.figure()\n",
    "                for line in plot_config['lines']:\n",
    "                    line_config = plot_config['lines'][line]\n",
    "                    x = 1-np.logspace(np.log10(tmin),np.log10(tmax),100)\n",
    "                    if len(data[line]) == 0: raise ValueError(line)\n",
    "                    y_raw = (np.array(round_err(data[line][spec][size][d]['dev']['test_score_values']))[None,:] >  x[:,None])\n",
    "                    y_mean = y_raw.mean(-1)\n",
    "                    y_sem= y_raw.std(-1) / np.sqrt(y_raw.shape[-1])\n",
    "                    plt.plot(x, y_mean, \n",
    "                                linestyle=line_config['linestyle'], \n",
    "                                color=line_config['color'], \n",
    "                                label=line_config['name'],\n",
    "                            #      elinewidth=1,     # thinner bar lines\n",
    "                            # capsize=3,        # half-length of the “T” caps\n",
    "                            # capthick=1,        # thickness of the caps\n",
    "                    )\n",
    "                    plt.fill_between(x, y_mean-y_sem, y_mean+y_sem, color=line_config['color'], alpha=plot_config['shade_alpha'])\n",
    "                plt.legend()\n",
    "                plt.xlabel('threshold')\n",
    "                plt.ylabel('accuracy')\n",
    "                plt.title(f'{spec} {d} (n={data[line][spec][size][d][\"dev\"][\"test_score_count\"]})')\n",
    "                plt.ylim(plot_config['ymin'], plot_config['ymax'])\n",
    "                plt.savefig(f'plots/dev.{spec}.{size}.{d}.{subname}.threshold.pdf')\n",
    "                plt.clf()\n",
    "\n",
    "ttable = 0.01\n",
    "table_rows = []\n",
    "table_row_headers = []\n",
    "table_column_headers = [line['short'] for line in plot_config['lines'].values()]\n",
    "\n",
    "for spec in ['n5v2', 'n10v2', 'n15v2']:\n",
    "    for size in ['2k']:\n",
    "        for d in ['d0', 'd1', 'd2', 'd3', 'd4', 'd5'] if spec != 'n5v2' else ['d0', 'd1', 'd2', 'd3']:\n",
    "            row = []\n",
    "            for line in plot_config['lines']:\n",
    "                line_config = plot_config['lines'][line]\n",
    "                y_raw = (np.array(round_err(data[line][spec][size][d]['dev']['test_score_values']))) > (1- ttable)\n",
    "                y_mean = y_raw.mean(-1)\n",
    "                y_sem= y_raw.std(-1) / np.sqrt(y_raw.shape[-1])\n",
    "                row.append((float(y_mean), 2*float(y_sem)))\n",
    "            table_rows.append(row)\n",
    "            table_row_headers.append(f'{spec}-{d}')\n",
    "print(latex_table(table_column_headers, table_row_headers, table_rows, times100=True, caption=\"RL vs SFT, accuracy (average \\textsc{correct}) at threshold $0.01$, see\\cref{eqn:correct}. System with the best overal mean is bolded. Range denotes 95\\\\% confidence interval.\", label=\"tab:rl_vs_sft\", table_font_size='tiny'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
